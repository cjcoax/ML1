{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Assignment 8: Dimensionality Reduction for Supervised Learning\n",
    "\n",
    "Previous version is from MLEARN 510 course materials in 2021, ML510-Assignment8-Solution.ipynb. <br>\n",
    "Modified and Extended by Ernst Henle.<br>\n",
    "Copyright Â© 2024 by Ernst Henle\n",
    "\n",
    "# Learning Objectives\n",
    "- Be able to make application decisions regarding principal component analysis to train and test data \n",
    "- Produce a dimensionality reduction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Data\n",
    "We will use the MNIST (\"Modified National Institute of Standards and Technology\") dataset to demonstrate dimensionality reduction for supervised learning.\n",
    "<br>\n",
    "The [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) is a standard dataset of 70000 images of hand-written digits.  Each image is 28-by-28 ($28 X 28 = 784$) pixels and contains one hand-written digit.  Each image occupies one row of the csv file or numpy array.  The first 60000 rows are training images.  The last 10000 rows are test images.  \n",
    "The dataset can be downloaded from many websites including Canvas as mnist_784.csv.  The most convenient source of the dataset is through `fetch_openml` in `sklearn.datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the MNIST dataset\n",
    "import time\n",
    "t0 = time.time()\n",
    "mnist = fetch_openml('mnist_784', parser='pandas')\n",
    "print(\"Data loading took {:.2f}s\".format(time.time() - t0))\n",
    "X_byte = mnist['data'].to_numpy()\n",
    "y = mnist['target'].to_numpy()\n",
    "mnist = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import pandas as pd\n",
    "# mnist_start_time = time.time()\n",
    "# mnist = pd.read_csv('../data/mnist_784.csv') # 14 sec\n",
    "# print(\"MNIST read elapsed time: \", time.time() - mnist_start_time)\n",
    "# X_byte = mnist.drop(columns=['class'], inplace=False).to_numpy()\n",
    "# y = mnist['class'].to_numpy()\n",
    "# mnist = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic EDA\n",
    "\n",
    "# Show the shapes of the training and test data sets\n",
    "print('Shape of input features is:', X_byte.shape, ' Shape of target(digits) is:', y.shape)\n",
    "print('Range of input features is from', X_byte.min(),'to', X_byte.max())\n",
    "\n",
    "# Show the distribution of digits\n",
    "print('###########\\n Distribution of digits:')\n",
    "labels, counts = np.unique(y, return_counts=True)\n",
    "display(pd.DataFrame([counts], columns=labels, index=['counts']))\n",
    "\n",
    "print('###########\\n Sample of input features:')\n",
    "display(X_byte[0:5,400:410])\n",
    "print('###########\\n')\n",
    "# Plot one of the images\n",
    "import matplotlib.pyplot as plt\n",
    "plt.gray()\n",
    "rand_i = np.random.randint(low=0, high=70000)\n",
    "plt.matshow(X_byte[rand_i,:].reshape((28,28)).astype(float));    \n",
    "plt.title(f'Digit: {y[rand_i]}')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "[Feature scaling](https://en.wikipedia.org/wiki/Feature_scaling) for image data is different than in many machine learning applications.  Often, the best scaling is simply by dividing by the max value of all features.  The following are explanations with examples: \n",
    "<br><br>\n",
    "#### Argument for normalizing each feature individually\n",
    "In many machine learning models the numeric input features are **not** of the same kind.  Given the description of a car, one may use the features weight, height, and age.  Although these three features are correlated, they mean different things and are on different scales.  For instance, it is meaningless to say that the age is bigger than the weight.  Also, when we combine these features, like we do in PCA, we will get a new kind of feature that is neither weight, height, nor age.  The purpose of normalization is to bring these very different features onto a similar scale prior to PCA.  Such normalization must be done individually, where the normalization factors are determined separtely for each feature.\n",
    "<br><br>\n",
    "#### Argument against normalizing each feature individually\n",
    "In some machine learning models the numeric input features **are** of the same kind.  The three input features for a box might be height, width and length.  All three input features are spatial dimensions and are on the same scale.  For instance, if we rotate the box we might switch the values of height and width.  When we combine spatial dimensions, as we do in PCA, then the result is still a spatial dimension.  If the features are already on the same units, then individual normalization may be counter productive.  If we do normalize, then all related features should be normalized with the same normalization parameters to preserve the relative differences between features.\n",
    "<br><br>\n",
    "#### General conclusion\n",
    "The conclusion is that in contrast to what we previously discussed about normalization, sometimes we should preserve the different ranges between features.\n",
    "<br><br>\n",
    "#### Feature scaling for our current dataset\n",
    "In our current dataset, all the image features are pixel values in the range from 0 to 255.  We can directly compare one pixel value to another and a combination of pixel values will result in a composite pixel value.  In this situation, it is best to preserve the different ranges between features.  We can either not normalize at all or we can simply divide all features by the maximum pixel value in the whole dataset.  Thus all features are on the same 0 to 1 scale but any given feature may have a minimum higher than 0 or a maximum lower than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the input features\n",
    "X_max = X_byte.max()\n",
    "X = X_byte/X_max\n",
    "\n",
    "# Remove X_byte so that it is not accidentally used\n",
    "X_byte = None\n",
    "\n",
    "# Present a sample of the scaled input features\n",
    "display(X[0:5,400:410])\n",
    "print('Range of scaled input features is from', X.min(),'to', X.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Question 1\n",
    "Split the data into a training set and a test set\n",
    "- the first 60,000 rows (images) are for training\n",
    "- the last 10,000 rows (images) are for testing).\n",
    "- show the shapes of the training and test data sets\n",
    "- show the distribution of digits in the test set.\n",
    "   - Has the distribution changed from the original dataset\n",
    "   - What are the consequences for testing on an uneven distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first 60000 rows are for training\n",
    "\n",
    "\n",
    "# The last 10000 rows are for testing\n",
    "\n",
    "\n",
    "# Show the shapes of the training and test data sets\n",
    "\n",
    "\n",
    "# Show the distribution of digits in the test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Train a Logistic Regression classifier on the dataset.\n",
    "- The argument list must indicate that you want to do a multinomial logistic regression.\n",
    "- Set  `max_iter` to 1000 (Before you set `max_iter` to 1000, you may want to test your code with `max_iter` set to 100 for faster debugging)  \n",
    "- Time the training using the `time` or `timeit` module and present the training time in seconds\n",
    "\n",
    "There is no need to predict on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multinomial logistic regression classifier\n",
    "\n",
    "\n",
    "# Present the time it took for training (just \".fit\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Evaluate the resulting model on the test set.  Determine the accuracy.  For these purposes Accuracy is defined as <br><center>***correct predictions / all_predictions***</center><br>  You can use the `.score` method from logistic regression or the `metrics.accuracy_score` from sklearn or some other method that calculates accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracy of model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Question 4\n",
    "Use PCA to analyze the data.  \n",
    "- Train PCA on training data\n",
    "- Present the explained variance (`.explained_variance_`) for each principal component in a scree plot\n",
    "- Determine the minimum number of components to get 95% of the explained variance.\n",
    "- Use the explained variance (`.explained_variance_`) to create a cumulative variance plot\n",
    "- Create a lower dimensional dataset that has 95% of the explained variance and present the shape of the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train PCA on training data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Scree Plot of explained variance\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine Cumulative Explained Variance\n",
    "\n",
    "\n",
    "# Determine number of principal components necessary for 95% of explained variance\n",
    "# Find Number of Principal Components Neccessary to Achieve Minimum Explained Variance\n",
    "\n",
    "\n",
    "# Plot Cumulative Explained Variance vs Principal Components\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reduced dataset that contains only the number of principal components necessary for 95% of explained variance\n",
    "\n",
    "\n",
    "# Present shape of reduced dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Question 5\n",
    "Train a new Logistic Regression classifier on the reduced training dataset.  Use the same parameters (arguments) as before. \n",
    "- As before, time the training\n",
    "- Was training much faster? Explain your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multinomial logistic regression classifier on the reduced dataset\n",
    "\n",
    "\n",
    "# Present the time it took for training (just \".fit\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "1. Evaluate the new classifier \n",
    "  - Transform the test data using the PCA model that was trained on the training data\n",
    "  - Remove the excess columns of the pca-transformed test data. \n",
    "  - Determine the accuracy of the PCR (logistic regression) on the test data with the same accuracy method as before.\n",
    "2. Discuss how the accuracy compares to the previous classifier.  Discuss the speed vs. accuracy trade-off and in which case you'd prefer a very slight drop in model performance for a x-time speedup in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform input features of test set according to training set PCA\n",
    "\n",
    "\n",
    "# Remove the excess columns of the pca-transformed test data\n",
    "\n",
    "\n",
    "# Use score method to get accuracy of model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "Create a new text cell in your Notebook: Complete a 50-100 word summary (or short description of your thinking in applying this week's learning to the solution) of your experience in this assignment. Include: \n",
    "- What was your incoming experience with this model, if any?\n",
    "- what steps you took\n",
    "- what obstacles you encountered\n",
    "- how you link this exercise to real-world, machine learning problem-solving. (What steps were missing? What else do you need to learn?) \n",
    "\n",
    "<br> <br>\n",
    "This summary allows your instructor to know how you are doing and allot points for your effort in thinking and planning, and making connections to real-world work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
