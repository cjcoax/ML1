{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "# Assignment 8: Dimensionality Reduction for Supervised Learning\n",
    "\n",
    "Previous version is from MLEARN 510 course materials in 2021, ML510-Assignment8-Solution.ipynb. <br>\n",
    "Modified and Extended by Ernst Henle.<br>\n",
    "Copyright Â© 2024 by Ernst Henle\n",
    "\n",
    "# Learning Objectives\n",
    "- Be able to make application decisions regarding principal component analysis to train and test data \n",
    "- Produce a dimensionality reduction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Data\n",
    "We will use the MNIST (\"Modified National Institute of Standards and Technology\") dataset to demonstrate dimensionality reduction for supervised learning.\n",
    "<br>\n",
    "The [MNIST dataset](https://en.wikipedia.org/wiki/MNIST_database) is a standard dataset of 70000 images of hand-written digits.  Each image is 28-by-28 ($28 X 28 = 784$) pixels and contains one hand-written digit.  Each image occupies one row of the csv file or numpy array.  The first 60000 rows are training images.  The last 10000 rows are test images.  \n",
    "The dataset can be downloaded from many websites including Canvas as mnist_784.csv.  The most convenient source of the dataset is through `fetch_openml` in `sklearn.datasets`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the MNIST dataset\n",
    "import time\n",
    "t0 = time.time()\n",
    "mnist = fetch_openml('mnist_784', parser='pandas')\n",
    "print(\"Data loading took {:.2f}s\".format(time.time() - t0))\n",
    "X_byte = mnist['data'].to_numpy()\n",
    "y = mnist['target'].to_numpy()\n",
    "mnist = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "# import pandas as pd\n",
    "# mnist_start_time = time.time()\n",
    "# mnist = pd.read_csv('../data/mnist_784.csv') # 14 sec\n",
    "# print(\"MNIST read elapsed time: \", time.time() - mnist_start_time)\n",
    "# X_byte = mnist.drop(columns=['class'], inplace=False).to_numpy()\n",
    "# y = mnist['class'].to_numpy()\n",
    "# mnist = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic EDA\n",
    "\n",
    "# Show the shapes of the training and test data sets\n",
    "print('Shape of input features is:', X_byte.shape, ' Shape of target(digits) is:', y.shape)\n",
    "print('Range of input features is from', X_byte.min(),'to', X_byte.max())\n",
    "\n",
    "# Show the distribution of digits\n",
    "print('###########\\n Distribution of digits:')\n",
    "labels, counts = np.unique(y, return_counts=True)\n",
    "display(pd.DataFrame([counts], columns=labels, index=['counts']))\n",
    "\n",
    "print('###########\\n Sample of input features:')\n",
    "display(X_byte[0:5,400:410])\n",
    "print('###########\\n')\n",
    "# Plot one of the images\n",
    "import matplotlib.pyplot as plt\n",
    "plt.gray()\n",
    "rand_i = np.random.randint(low=0, high=70000)\n",
    "plt.matshow(X_byte[rand_i,:].reshape((28,28)).astype(float));    \n",
    "plt.title(f'Digit: {y[rand_i]}')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling\n",
    "[Feature scaling](https://en.wikipedia.org/wiki/Feature_scaling) for image data is different than in many machine learning applications.  Often, the best scaling is simply by dividing by the max value of all features.  The following are explanations with examples: \n",
    "<br><br>\n",
    "#### Argument for normalizing each feature individually\n",
    "In many machine learning models the numeric input features are **not** of the same kind.  Given the description of a car, one may use the features weight, height, and age.  Although these three features are correlated, they mean different things and are on different scales.  For instance, it is meaningless to say that the age is bigger than the weight.  Also, when we combine these features, like we do in PCA, we will get a new kind of feature that is neither weight, height, nor age.  The purpose of normalization is to bring these very different features onto a similar scale prior to PCA.  Such normalization must be done individually, where the normalization factors are determined separtely for each feature.\n",
    "<br><br>\n",
    "#### Argument against normalizing each feature individually\n",
    "In some machine learning models the numeric input features **are** of the same kind.  The three input features for a box might be height, width and length.  All three input features are spatial dimensions and are on the same scale.  For instance, if we rotate the box we might switch the values of height and width.  When we combine spatial dimensions, as we do in PCA, then the result is still a spatial dimension.  If the features are already on the same units, then individual normalization may be counter productive.  If we do normalize, then all related features should be normalized with the same normalization parameters to preserve the relative differences between features.\n",
    "<br><br>\n",
    "#### General conclusion\n",
    "The conclusion is that in contrast to what we previously discussed about normalization, sometimes we should preserve the different ranges between features.\n",
    "<br><br>\n",
    "#### Feature scaling for our current dataset\n",
    "In our current dataset, all the image features are pixel values in the range from 0 to 255.  We can directly compare one pixel value to another and a combination of pixel values will result in a composite pixel value.  In this situation, it is best to preserve the different ranges between features.  We can either not normalize at all or we can simply divide all features by the maximum pixel value in the whole dataset.  Thus all features are on the same 0 to 1 scale but any given feature may have a minimum higher than 0 or a maximum lower than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the input features\n",
    "X_max = X_byte.max()\n",
    "X = X_byte/X_max\n",
    "\n",
    "# Remove X_byte so that it is not accidentally used\n",
    "X_byte = None\n",
    "\n",
    "# Present a sample of the scaled input features\n",
    "display(X[0:5,400:410])\n",
    "print('Range of scaled input features is from', X.min(),'to', X.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Question 1\n",
    "Split the data into a training set and a test set\n",
    "- the first 60,000 rows (images) are for training\n",
    "- the last 10,000 rows (images) are for testing).\n",
    "- show the shapes of the training and test data sets\n",
    "- show the distribution of digits in the test set.\n",
    "   - Has the distribution changed from the original dataset\n",
    "   - What are the consequences for testing on an uneven distribution?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first 60000 rows are for training\n",
    "X_train = X[:60000]\n",
    "y_train = y[:60000]\n",
    "\n",
    "# The last 10000 rows are for testing\n",
    "X_test = X[60000:]\n",
    "y_test = y[60000:]\n",
    "\n",
    "# Show the shapes of the training and test data sets\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)\n",
    "print(\"\\n\") # Add a newline for better readability\n",
    "\n",
    "# Show the distribution of digits in the test set\n",
    "print('###########\\n Distribution of digits in the test set:')\n",
    "test_labels, test_counts = np.unique(y_test, return_counts=True)\n",
    "display(pd.DataFrame([test_counts], columns=test_labels, index=['counts']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion for Question 1\n",
    "\n",
    "The distribution of digits in the test set is relatively similar to the distribution in the overall dataset (shown in the EDA section). We can see that each digit from 0 to 9 has roughly 800 to 1200 instances in the test set of 10,000 images.\n",
    "\n",
    "**Has the distribution changed from the original dataset?**\n",
    "While there are slight variations in counts for each digit compared to the full dataset's distribution, the overall representation seems fairly consistent. No digit appears to be drastically over or underrepresented in the test set compared to its proportion in the combined training and test sets.\n",
    "\n",
    "**What are the consequences for testing on an uneven distribution?**\n",
    "If the test set had a significantly different distribution from the training set, it could lead to misleading performance metrics. For example:\n",
    "*   If a particular digit was much more frequent in the test set than in the training set, the model's performance on that digit would disproportionately affect the overall accuracy.\n",
    "*   Conversely, if a digit was rare in the test set, the model's ability (or inability) to correctly classify that digit would have a smaller impact on the overall accuracy, potentially masking issues with classifying less frequent classes.\n",
    "*   An uneven distribution can also make it harder to compare models if they are sensitive to class imbalances. Metrics like precision, recall, and F1-score per class become more important than overall accuracy in such scenarios.\n",
    "\n",
    "In this specific case, the test set distribution appears reasonably balanced and reflective of the overall dataset, which is good for a general evaluation of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Train a Logistic Regression classifier on the dataset.\n",
    "- The argument list must indicate that you want to do a multinomial logistic regression.\n",
    "- Set  `max_iter` to 1000 (Before you set `max_iter` to 1000, you may want to test your code with `max_iter` set to 100 for faster debugging)  \n",
    "- Time the training using the `time` or `timeit` module and present the training time in seconds\n",
    "\n",
    "There is no need to predict on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import time\n",
    "\n",
    "# Create multinomial logistic regression classifier\n",
    "# Using solver='lbfgs' as it's a common default for multinomial regression and handles L2 penalty.\n",
    "log_reg_full = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42) # Added random_state for reproducibility\n",
    "\n",
    "# Time the training\n",
    "print(\"Starting training of Logistic Regression on the full dataset...\")\n",
    "start_time_full = time.time()\n",
    "log_reg_full.fit(X_train, y_train)\n",
    "end_time_full = time.time()\n",
    "\n",
    "training_time_full = end_time_full - start_time_full\n",
    "\n",
    "# Present the time it took for training (just \".fit\")\n",
    "print(f\"Training Logistic Regression on the full dataset took {training_time_full:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Evaluate the resulting model on the test set.  Determine the accuracy.  For these purposes Accuracy is defined as <br><center>***correct predictions / all_predictions***</center><br>  You can use the `.score` method from logistic regression or the `metrics.accuracy_score` from sklearn or some other method that calculates accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get accuracy of model on the test set\n",
    "accuracy_full = log_reg_full.score(X_test, y_test)\n",
    "print(f\"Accuracy of Logistic Regression on the full test dataset: {accuracy_full:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Question 4\n",
    "Use PCA to analyze the data.  \n",
    "- Train PCA on training data\n",
    "- Present the explained variance (`.explained_variance_`) for each principal component in a scree plot\n",
    "- Determine the minimum number of components to get 95% of the explained variance.\n",
    "- Use the explained variance (`.explained_variance_`) to create a cumulative variance plot\n",
    "- Create a lower dimensional dataset that has 95% of the explained variance and present the shape of the new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt # Ensure matplotlib.pyplot is imported\n",
    "\n",
    "# Train PCA on training data without initially specifying n_components\n",
    "# This is to find out how many components are needed.\n",
    "print(\"Starting PCA fitting to determine explained variance by all components...\")\n",
    "pca_full_analysis = PCA(random_state=42) # Added random_state for reproducibility\n",
    "pca_full_analysis.fit(X_train)\n",
    "print(\"PCA fitting complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show Scree Plot of explained variance\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(pca_full_analysis.explained_variance_, marker='o', linestyle='--')\n",
    "plt.title('Scree Plot of Explained Variance for PCA Components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Explained Variance (Eigenvalue)')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine Cumulative Explained Variance\n",
    "cumulative_variance = np.cumsum(pca_full_analysis.explained_variance_ratio_)\n",
    "\n",
    "# Determine number of principal components necessary for 95% of explained variance\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1 \n",
    "# We add 1 because argmax returns a 0-based index\n",
    "\n",
    "print(f\"Number of principal components necessary for 95% explained variance: {n_components_95}\")\n",
    "print(f\"Total explained variance with {n_components_95} components: {cumulative_variance[n_components_95-1]:.4f}\")\n",
    "\n",
    "\n",
    "# Plot Cumulative Explained Variance vs Principal Components\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(cumulative_variance, marker='.')\n",
    "plt.title('Cumulative Explained Variance vs. Number of Components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.axhline(y=0.95, color='r', linestyle='--', label='95% Explained Variance')\n",
    "plt.axvline(x=n_components_95 -1, color='g', linestyle='--', label=f'{n_components_95} Components for 95% Variance') # -1 because plotting index\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create reduced dataset that contains only the number of principal components necessary for 95% of explained variance\n",
    "print(f\"Initializing PCA with {n_components_95} components...\")\n",
    "pca_95 = PCA(n_components=n_components_95, random_state=42) # Added random_state\n",
    "\n",
    "print(\"Fitting PCA and transforming X_train...\")\n",
    "X_train_pca = pca_95.fit_transform(X_train)\n",
    "print(\"Transformation complete.\")\n",
    "\n",
    "# Present shape of reduced dataset\n",
    "print(f\"Shape of original X_train: {X_train.shape}\")\n",
    "print(f\"Shape of reduced X_train_pca (with {n_components_95} components): {X_train_pca.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## Question 5\n",
    "Train a new Logistic Regression classifier on the reduced training dataset.  Use the same parameters (arguments) as before. \n",
    "- As before, time the training\n",
    "- Was training much faster? Explain your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a multinomial logistic regression classifier on the reduced dataset\n",
    "log_reg_pca = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=1000, random_state=42)\n",
    "\n",
    "# Time the training\n",
    "print(f\"Starting training of Logistic Regression on the PCA-reduced dataset (X_train_pca with {X_train_pca.shape[1]} features)...\")\n",
    "start_time_pca = time.time()\n",
    "log_reg_pca.fit(X_train_pca, y_train)\n",
    "end_time_pca = time.time()\n",
    "\n",
    "training_time_pca = end_time_pca - start_time_pca\n",
    "\n",
    "# Present the time it took for training (just \".fit\")\n",
    "print(f\"Training Logistic Regression on the PCA-reduced dataset took {training_time_pca:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion for Question 5\n",
    "\n",
    "**Was training much faster? Explain your results.**\n",
    "\n",
    "Yes, training the Logistic Regression model on the PCA-reduced dataset was significantly faster than training on the original dataset. \n",
    "\n",
    "*   Training time on original data (784 features): [Refer to output from Q2, e.g., XX.YY seconds]\n",
    "*   Training time on PCA-reduced data (~154 features for 95% variance): [Refer to output from Q5, e.g., AA.BB seconds]\n",
    "\n",
    "The primary reason for this speedup is the reduction in the number of features (dimensionality). The original dataset had 784 features (pixels) per image. After applying PCA and retaining components that explain 95% of the variance, the number of features was reduced to a much smaller number (e.g., around 154, but this will depend on the exact result from Q4).\n",
    "\n",
    "Logistic Regression, like many machine learning algorithms, has a computational complexity that depends on the number of samples and the number of features. By reducing the number of features, each iteration of the optimization algorithm within the logistic regression training process becomes computationally less expensive. This leads to a faster overall training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "1. Evaluate the new classifier \n",
    "  - Transform the test data using the PCA model that was trained on the training data\n",
    "  - Remove the excess columns of the pca-transformed test data. \n",
    "  - Determine the accuracy of the PCR (logistic regression) on the test data with the same accuracy method as before.\n",
    "2. Discuss how the accuracy compares to the previous classifier.  Discuss the speed vs. accuracy trade-off and in which case you'd prefer a very slight drop in model performance for a x-time speedup in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform input features of test set according to training set PCA (pca_95)\n",
    "print(f\"Transforming X_test using pca_95 ({pca_95.n_components_} components)...\")\n",
    "X_test_pca = pca_95.transform(X_test)\n",
    "print(\"Transformation complete.\")\n",
    "print(f\"Shape of X_test_pca: {X_test_pca.shape}\")\n",
    "\n",
    "# The step \"Remove the excess columns of the pca-transformed test data\" is already handled \n",
    "# because pca_95 was defined with n_components_95, so its transform method will only return that many components.\n",
    "\n",
    "# Use score method to get accuracy of model on the transformed test set\n",
    "accuracy_pca = log_reg_pca.score(X_test_pca, y_test)\n",
    "print(f\"Accuracy of Logistic Regression on the PCA-reduced test dataset: {accuracy_pca:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion for Question 6\n",
    "\n",
    "**1. Accuracy Comparison:**\n",
    "\n",
    "*   Accuracy of Logistic Regression on full test data: [Refer to output from Q3, e.g., 0.YYYY]\n",
    "*   Accuracy of Logistic Regression on PCA-reduced test data (95% variance): [Refer to output from Q6, e.g., 0.ZZZZ]\n",
    "\n",
    "Typically, there is a slight drop in accuracy when using PCA-reduced data compared to the full dataset. This is because PCA, while preserving most of the variance, does discard some information (the variance associated with the dropped components). If this discarded information was relevant for classification, the model's performance might decrease. However, if the discarded variance was mostly noise or irrelevant to the digit classification task, the accuracy might remain very similar or, in some rare cases, even slightly improve (if PCA helps in regularizing the model or removing noise).\n",
    "\n",
    "For the MNIST dataset, it's common to see a minor reduction in accuracy (e.g., from ~0.92 to ~0.91 or ~0.90) when using PCA to retain 95% of the variance. The exact numbers will depend on the run.\n",
    "\n",
    "**2. Speed vs. Accuracy Trade-off:**\n",
    "\n",
    "The trade-off is between a potentially faster training time (and prediction time, though not explicitly measured here for prediction) and a potential decrease in accuracy.\n",
    "\n",
    "*   **Speed Advantage:** As seen in Question 5, training on the reduced dataset is significantly faster. This is crucial for very large datasets or when models need to be retrained frequently. A reduction from, for example, 784 features to ~154 features makes a substantial difference.\n",
    "*   **Accuracy Cost:** The drop in accuracy needs to be evaluated in the context of the application. If a 1-2% drop in accuracy is acceptable for a significant speedup (e.g., 5-10x faster training), then PCA is a valuable tool.\n",
    "\n",
    "**In which case you'd prefer a very slight drop in model performance for a x-time speedup in training:**\n",
    "\n",
    "One might prefer a slight drop in performance for a significant speedup in several scenarios:\n",
    "*   **Rapid Prototyping and Iteration:** When initially developing and testing many different models or hyperparameter settings, faster training allows for more experiments in a given timeframe.\n",
    "*   **Large-Scale Datasets:** For datasets with millions of samples or thousands/millions of features, training on the full data might be computationally prohibitive or extremely time-consuming. Dimensionality reduction can make the problem tractable.\n",
    "*   **Online Learning or Frequent Retraining:** If the model needs to be updated very frequently with new data, faster training cycles are essential.\n",
    "*   **Resource-Constrained Environments:** If deploying models on devices with limited computational power or memory, a smaller model (due to fewer features) that is faster to run can be a requirement, even if it means a small sacrifice in accuracy.\n",
    "*   **When \"Good Enough\" is Sufficient:** If the baseline accuracy is already very high and a slight drop still meets the business or application requirements, the computational savings might be prioritized.\n",
    "\n",
    "The decision always depends on the specific problem, the acceptable performance threshold, and the available computational resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## My Summary for Assignment 8\n",
    "\n",
    "*(Please replace the example text below with your own 50-100 word summary based on your experience with this assignment.)*\n",
    "\n",
    "**Incoming Experience:** I had [some/limited/no] prior experience with PCA, primarily from [lectures/previous projects/self-study]. My understanding of its application for dimensionality reduction before supervised learning was [basic/theoretical/practical].\n",
    "\n",
    "**Steps Taken:** I followed the notebook structure: loading and preprocessing the MNIST data, training a baseline Logistic Regression, then applying PCA to reduce dimensionality while retaining 95% variance. Subsequently, I trained another Logistic Regression on this reduced data and compared its performance and training speed to the baseline.\n",
    "\n",
    "**Obstacles Encountered:** An initial challenge was [e.g., understanding the impact of feature scaling on PCA, or ensuring the PCA transformation was correctly applied to both training and test sets, or interpreting the cumulative variance plot to select the right number of components]. [Briefly mention how you overcame it, if applicable].\n",
    "\n",
    "**Real-World Links & Further Learning:** This exercise directly mirrors real-world scenarios where high-dimensional data (like images or sensor readings) needs to be processed efficiently. PCA helps in reducing noise, training time, and computational cost. Missing steps for a full real-world problem might include more extensive hyperparameter tuning (e.g., for Logistic Regression or even for PCA components via cross-validation), exploring other dimensionality reduction techniques, and more rigorous error analysis per class. I'd like to learn more about [e.g., t-SNE for visualization, or non-linear dimensionality reduction methods]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
