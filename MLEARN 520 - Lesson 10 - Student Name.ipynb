{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFLOW - Deploying Machine Learning in Production\n",
    "\n",
    "In this assignment you will be writing a script that train models and use `mlflow` to submit runs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./new_data.json\n",
    "{\"age\": {\"0\": 40, \"1\": 47},\n",
    " \"balance\": {\"0\": 580, \"1\": 3644},\n",
    " \"campaign\": {\"0\": 1, \"1\": 2},\n",
    " \"contact\": {\"0\": \"unknown\", \"1\": \"unknown\"},\n",
    " \"day\": {\"0\": 16, \"1\": 9},\n",
    " \"default\": {\"0\": \"no\", \"1\": \"no\"},\n",
    " \"duration\": {\"0\": 192, \"1\": 83},\n",
    " \"education\": {\"0\": \"secondary\", \"1\": \"secondary\"},\n",
    " \"housing\": {\"0\": \"yes\", \"1\": \"no\"},\n",
    " \"job\": {\"0\": \"blue-collar\", \"1\": \"services\"},\n",
    " \"loan\": {\"0\": \"no\", \"1\": \"no\"},\n",
    " \"marital\": {\"0\": \"married\", \"1\": \"single\"},\n",
    " \"month\": {\"0\": \"may\", \"1\": \"jun\"},\n",
    " \"pdays\": {\"0\": -1, \"1\": -1},\n",
    " \"poutcome\": {\"0\": \"unknown\", \"1\": \"unknown\"},\n",
    " \"previous\": {\"0\": 0, \"1\": 0}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load all necessary libraries\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# Load Dataset\n",
    "bank = pd.read_csv('bank-full.csv', delimiter = ';')\n",
    "\n",
    "# Split data between train and validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(bank.drop(columns = \"y\"), bank[\"y\"], \n",
    "                                                    test_size = 0.10, random_state = 42)\n",
    "\n",
    "X_train = X_train.reset_index(drop = True)\n",
    "X_test = X_test.reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 1: Create pre-processing function to be later used as part of the pipeline (custom transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformations(df):\n",
    "    \"\"\"Apply one-hot encoding to categorical columns and\n",
    "    standard scaling to numeric columns.\"\"\"\n",
    "    # One-hot encode categorical features\n",
    "    onehoter = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "    cat_cols = X_train.select_dtypes(['object']).columns\n",
    "    onehoter.fit(X_train[cat_cols])\n",
    "    onehot_cols = onehoter.get_feature_names_out(cat_cols)\n",
    "    df_onehot = pd.DataFrame(\n",
    "        onehoter.transform(df[cat_cols]), columns=onehot_cols\n",
    "    )\n",
    "\n",
    "    # Standardize numerical features\n",
    "    num_cols = X_train.select_dtypes(['integer', 'float']).columns\n",
    "    znormalizer = StandardScaler()\n",
    "    znormalizer.fit(X_train[num_cols])\n",
    "    df_norm = pd.DataFrame(\n",
    "        znormalizer.transform(df[num_cols]), columns=num_cols\n",
    "    )\n",
    "\n",
    "    # Combine processed categorical and numerical data\n",
    "    df_featurized = df_onehot\n",
    "    df_featurized[num_cols] = df_norm\n",
    "\n",
    "    # Clean up intermediate dataframes\n",
    "    del df_onehot, df_norm\n",
    "    return df_featurized\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 2: Creating a custom transformer from the previously defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the preprocessing function so it can be used in a pipeline\n",
    "pre_processing = FunctionTransformer(transformations, validate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3: Creating the pipeline and defining each of two steps: (i) pre-processing, and; (ii) model (Logistic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('pre_processing', pre_processing),  # feature engineering step\n",
    "    ('model', LogisticRegression())      # classification model\n",
    "], verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 4: Call `fit` and `predict` on the pipeline to make sure that it all works. Remember to pass them the **un-processed** (original) data, since the data processing should be built into the pipeline now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for Logistic Regression estimator ('model') inside the pipeline\n",
    "pipeline.set_params(model__C=1.0,                 # strength of regularization\n",
    "                    model__solver='lbfgs',        # optimization algorithm\n",
    "                    model__max_iter=200,          # increase max_iter for convergence\n",
    "                    model__fit_intercept=True,    # include intercept term\n",
    "                    model__penalty='l2')          # regularization penalty type\n",
    "\n",
    "# Fit Training Data to Model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Prediction on Training and Test Data\n",
    "y_train_pred = pipeline.predict(X_train)\n",
    "y_test_pred = pipeline.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 5: Evaluate your model by calculating the precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to evaluate the model performance using precision and recall\n",
    "def eval_metrics(actual, pred):\n",
    "    precision = precision_score(actual, pred, pos_label='yes')\n",
    "    recall = recall_score(actual, pred, pos_label='yes')\n",
    "\n",
    "    return precision, recall\n",
    "\n",
    "# Calculation of evaluation metrics - Precision and Recall for training and validation data\n",
    "(precision_train, recall_train) = eval_metrics(y_train, y_train_pred)\n",
    "(precision_test, recall_test) = eval_metrics(y_test, y_test_pred)\n",
    "\n",
    "# Print Model (Logistic Regression) parameters\n",
    "print()\n",
    "print('Main Parameters used in logistic regression are: C={}, solver={}, max_iter={}, fit_intercept={} and penalty={}'.format(\n",
    "    pipeline['model'].get_params()['C'],\n",
    "    pipeline['model'].get_params()['solver'],\n",
    "    pipeline['model'].get_params()['max_iter'],\n",
    "    pipeline['model'].get_params()['fit_intercept'],\n",
    "    pipeline['model'].get_params()['penalty']\n",
    "))\n",
    "print('Training Precision: {:.3f}, Recall: {:.3f}'.format(precision_train, recall_train))\n",
    "print('Validation Precision: {:.3f}, Recall: {:.3f}'.format(precision_test, recall_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 6: Save your pipeline object using `joblib` as shown [here](https://sklearn.org/modules/model_persistence.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store 'pipeline' as pickle file using joblib\n",
    "joblib.dump(pipeline, 'pipeline.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 7: Now write a **new script** for scoring: it loads the pipeline you saved in the last step, reads the data `../data/new_data.json` and converts it to a `pandas.DataFrame` object, and obtains predictions on it. The predictions should be stored as a `json` file `../data/new_preds.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call and load stored 'pipeline'\n",
    "pipeline = joblib.load('pipeline.pkl')\n",
    "\n",
    "# Read json file with new data and write into a pandas dataframe\n",
    "with open('./new_data.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "new_predictions = pd.DataFrame(data)\n",
    "\n",
    "# Use predict method of pipeline to score (make prediction) on new data\n",
    "new_predictions['prediction'] = pipeline.predict(new_predictions)\n",
    "\n",
    "# Write predictions of new data into a json file\n",
    "new_predictions.to_json('./new_preds.json', orient='columns')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read json file containing predictions made for the new data and load them into a dataframe\n",
    "with open('./new_preds.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "new_pred_dataframe= pd.DataFrame(data)\n",
    "\n",
    "#Print predictions for each observation contained in the new_data.json file and the dataframe with the data and prediction\n",
    "print(new_pred_dataframe['prediction'])\n",
    "new_pred_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 8: Create a new text cell in your Notebook: Complete a 50-100 word summary (or short description of your thinking in applying this week's learning to the solution) of your experience in this assignment. Include: What was your incoming experience with this model, if any? what steps you took, what obstacles you encountered. how you link this exercise to real-world, machine learning problem-solving. (What steps were missing? What else do you need to learn?) This summary allows your instructor to know how you are doing and allot points for your effort in thinking and planning, and making connections to real-world work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise I built a logistic regression pipeline for the bank marketing dataset. My past experience with this model was limited, so I carefully created preprocessing steps with one-hot encoding and scaling, then wrapped them in a FunctionTransformer. The pipeline structure simplified training and evaluation. The main obstacle was keeping track of categorical vs. numeric features. This mirrors real projects where reproducible pipelines and metrics are essential for deployment.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}